\section{The AdaBoost algorithm}
\label{sec:bdt:ada}
The Adaptive Boost, AdaBoost, algorithm\footnote{
  This section is based on \Ref{AdaBoost}} negates the effect of statistical
fluctuations in a data set by increasing the weights of misclassified events.
The algorithm begins by training a single \DT as described above, where each event has unit weight.
For subsequent \DTs, the weight for each event, $i$, is modified for a tree $t$, to be
\begin{equation}
  w_i^t = c_i^t \times w_i^{t-1},
  \label{eq:ada:wt}
\end{equation}
where $c$ is determined to be
\begin{align}
  c_i^t &= \exp\big(\alpha_t\gamma_i^t\big),
  &&
  \gamma_i^t=\left\{
    \begin{array}{l}
      0\quad\text{event $i$ classified incorrectly by tree $t-1$} \\
      1\quad\text{event $i$ classified correctly by tree $t-1$}
    \end{array}
  \right.
\end{align}
%Here, $\gamma_i^t$ is unity if event $i$ was classified incorrectly in tree $t-1$, otherwise it is
%zero.
The value of $\alpha_t$ is the weight that the \DT carries, and is given by
\begin{equation}
  \alpha_t = \frac12\ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right)
\end{equation}
where $\epsilon$ is the weighted error rate.
Weights are then renormalised such that they sum to unity.
This reweighting procedure artificially fluctuates the training sample which is used to train
each \DT.
Multiple \DTs are made in this fashion, forming a forest; where the response of the \BDT classifier
is a combination of responses from all \DTs in the forest.
The total response of a \BDT, $T$, is
%for an event characterized by $x_i$, is
\begin{equation}
  T(x_i) = \sum_{t=1}^{n} \alpha_tT_t(x_i)
  \label{eq:ada:fullbdt}
\end{equation}
where $T_t(x_i)$ is the response of tree $t$, which returns one if it classifies $x_i$ as being
signal-like, and negative one if it is background-like.

The AdaBoost algorithm is fast, and often used in \gls{HEP} analyses.
It is implemented in \Chap{ch:hhh} to distinguish signal \btokpipimumu and \btophikmumu candidates
from combinatorial backgrounds.


%For a training sample, described by the varibles $x_i$ with true responses $f(x_i)\in{-1,1}$,
%a \BDT produced with the AdaBoost algorithm gives the output of
%\begin{equation}
  %T_m = \sum_{j=1}^{m}\alpha_jh_j(x_i),
%\end{equation}
%where $h_j(x_i)$ is the responce of a single \DT.
%The coefficent $\alpha_j$, is the weight given to \DT $j$, which is calculated as
%\begin{equation}
  %\alpha_j = \frac12\ln\left(\frac{1-\epsilon_j}{\epsilon_j}\right)
%\end{equation}
%where $\epsilon_j$ is the weighted error for a single \DT
%\begin{equation}
  %%\epsilon_j = \frac{\sum_{y_i\neq h(x_i)}w_i}{\sum_{i=1} h(x_i)w_i}.
  %\epsilon_j = \frac{W_\mathrm{misclassified}}{W_\mathrm{total}}
%\end{equation}
%Each event in the training sample is given a weight which changes as the \B\DT is trained.
%Initially each event has unit weight, $w_i^{(1)}=1$, but this changes to
%\begin{equation}
  %w_i^{(m)} = e^{-y_iC_m(x_i)},
%\end{equation}
%for subsequent \DTs, which boosts, thus boosting the importance of misclassified events.
%Decision tree number $m+1$ is then calculated using the above equations, and added to the total
%\B\DT.


