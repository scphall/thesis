\section{The bagging algorithm}
\label{sec:bdt:bag}
Bootstrap aggregating, or bagging\footnote{
  This section is based on \Ref{Bagging}.}, is a method of boosting whereby the effects of
statistical fluctuations are negated by making many independent \DTs and using them
to make a decision based on the average response.
%Assume a true distribution, $f(x_i)$, where $x_i$ are the training variables.
Training $n$ \DTs, it is possible to define three errors, namely:
square of the error of a single estimator
%The hypothesised value, from the output of a single \DT, is $h_t(x_i)$, for $1<t<n$, where
%$n$ is the number of \DTs and each \DT has a weight, $w_\alpha$ associated with it.
%Using these definitions, it is possible to define three errors, namely:
%square of the error of a single estimator
\begin{equation}
  \epsilon_t(x_i) = \big(f(x_i)-h_t(x_i)\big)^2
  \label{eq:bdt:bag1}
\end{equation}
where the index $t$ denotes a single \DT;
the average of individual squared errors
\begin{equation}
  \bar\epsilon(x_i) = \frac1n\sum_{t=1}^n\epsilon_t(x_i);
  \label{eq:bdt:bag2}
\end{equation}
and the squared error of an ensemble of \DTs
\begin{equation}
  e(x_i) = \big(f(x_i)-\bar h(x_i)\big)^2.
  \label{eq:bdt:bag3}
\end{equation}
The weighted variance of response of the estimators $h_t$ around a weighted mean is defined as
\begin{equation}
  V(x_i) = \frac1n\sum_{t=1}^n\big(h_t(x_i) - \bar h(x_i)\big)^2.
  \label{eq:bdt:bag4}
\end{equation}
%By inserting $f(x_i)-f(x_i)$ into \Eq{eq:bdt:bag4}, manipulating the algebra, the relationship
After some basic algebraic manipulation of \Eq{eq:bdt:bag4}, the relationship
\begin{equation}
  e(x_i) = \bar\epsilon(x_i) - V(x_i)
  \label{eq:bdt:bag5}
\end{equation}
emerges.
This means that the error squared of the ensemble of \DTs is equal to the average error squared of an
individual estimator, minus the weighted variance.
Therefore the process of bagging reduces the
effect of statistical fluctuations in the training samples~\cite{Krogh95neuralnetwork}.

A bagged \BDT is trained by randomly selecting events, with replacement, to train a single \DT.
Hundreds of \DTs can then be trained, and the weighted response from all \DTs is the result of the
classifier, in the range zero to one.

The bagging algorithm is used to train two \glspl{BDT} in \Chap{ch:dsphi}, one to identify the \Ds
meson, and the other to identify the \phii meson, above background combinatorics.
Both these BDTs are trained using a large number, $\mathcal{O}(50)$ variables, and therefore the
simplicity of bagging is advantageous.
Also, because the analysis uses two \glspl{BDT}, it is better to cut on the product of the \BDT
responses, which is more natural in the case of the bagging algorithm because the response is
between zero and one.


%Random selection of
%samples (with replacement) features (without replacement)
%During bootstrap approximately 1 − 1/e ≈ 2/3 samples are retained and 1/e ≈ 1/3 samples left out



