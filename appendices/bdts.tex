\chapter{BDTs}

The analyses detailed in \bam{Sections} of this thesis, make prodigous use of multivariate
techniques for the removal of combinatorial backgrounds.
Combinatorial background is fromed from a random combination of tracks that happen to form a well
defined vertex and pass selection criterea.
To remove these backgrounds multivariate selection techniques can be emplyed to exploit
correlations between variables in weakly discriminating variables to produce a single classifier
that can distinguish them from genunie signal.

A Decision Tree (DT) is a tree-like graph of decisions where, in this case, the edges correspond to
cuts on training variables and the final nodes are deemed to result in signal- of
background-like events.

A single DT is formed using a sample of signal and background candidates and a list of training
variables which might weakly separate the two.
A cut on a training variable is chosen to best separate the signal from the backgtround sample.
Each candidate is then defined as being most signal- or background-like depending on this cut.
This process can continue, splitting the sample down further, until...

\section{Bagging and boosting}
This algorithm will produce a DT whose output is weakly correlated to the true distribution of
candidates.
In order to increse the correlation, the technique of boosting is applied.
The process of boosting with reference to a DT is to weight events that are misclassified by the
DT, and then retrain the DT taking them into account.
There are are many varieties of boosting,






The boosted decision tree (BDT) is a machine
\cite{AdaBoost}
